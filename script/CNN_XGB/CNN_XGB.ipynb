{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1699415c-8b40-418d-a308-6125a4886f49",
   "metadata": {},
   "source": [
    "# CNN XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab1694-b612-4cbb-9880-3b35cc50c1a5",
   "metadata": {},
   "source": [
    "Author: Tiankai Yan <br>\n",
    "Date: 11/07/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29b86dad-3dec-437a-b382-e8825d4da04b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T06:06:14.679555Z",
     "iopub.status.busy": "2024-11-09T06:06:14.679280Z",
     "iopub.status.idle": "2024-11-09T06:06:24.069316Z",
     "shell.execute_reply": "2024-11-09T06:06:24.068779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kelvi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# system\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# deep learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db5fc50-5731-478c-b776-1e103bf6aa6c",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f75e7-934e-466b-bd95-a5e1cd54bbde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T06:06:24.080697Z",
     "iopub.status.busy": "2024-11-09T06:06:24.080380Z",
     "iopub.status.idle": "2024-11-09T06:06:31.612430Z",
     "shell.execute_reply": "2024-11-09T06:06:31.611895Z"
    }
   },
   "outputs": [],
   "source": [
    "with gzip.open(file_path, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629b91ca-cfa5-403b-8a78-7a7001491853",
   "metadata": {},
   "source": [
    "## Train-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc78fb8c-b7c4-4817-8627-9cefe01c1f72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T06:06:33.007329Z",
     "iopub.status.busy": "2024-11-09T06:06:33.006996Z",
     "iopub.status.idle": "2024-11-09T06:06:34.170903Z",
     "shell.execute_reply": "2024-11-09T06:06:34.170384Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.stack(data['upstream_region_encoded'].values)\n",
    "Y = data['DE'].values\n",
    "X_train, X_dev, Y_train, Y_dev = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=123)\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_dev_flat = X_dev.reshape(X_dev.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557a5990-36ec-4a3e-8895-c424fcd815aa",
   "metadata": {},
   "source": [
    "CNN PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36f5b6f6-6c21-4567-bcad-d3069071c45c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T06:06:34.179064Z",
     "iopub.status.busy": "2024-11-09T06:06:34.178588Z",
     "iopub.status.idle": "2024-11-09T06:06:34.181447Z",
     "shell.execute_reply": "2024-11-09T06:06:34.180978Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_dev = np.expand_dims(X_dev, axis=-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896489f-bb8c-4f9e-96f3-064c350dc5a8",
   "metadata": {},
   "source": [
    "Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "409306d7-897d-4558-b8c1-79993f1c5c55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T06:06:34.188659Z",
     "iopub.status.busy": "2024-11-09T06:06:34.188284Z",
     "iopub.status.idle": "2024-11-09T06:06:34.190874Z",
     "shell.execute_reply": "2024-11-09T06:06:34.190416Z"
    }
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 256\n",
    "EPOCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f78931-61ef-4bf3-9d65-a0dabdb715fa",
   "metadata": {},
   "source": [
    "Shuffle the dataset and apply mini-batch gradient descent with a batch size of 256:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a715f04c-b027-4976-9ba5-e8ef0ac2037f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T06:06:34.193007Z",
     "iopub.status.busy": "2024-11-09T06:06:34.192593Z",
     "iopub.status.idle": "2024-11-09T06:06:40.327596Z",
     "shell.execute_reply": "2024-11-09T06:06:40.327087Z"
    }
   },
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "dev = tf.data.Dataset.from_tensor_slices((X_dev, Y_dev)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58a4914f-f6ee-4a04-b058-60111dbc4085",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T06:06:40.330124Z",
     "iopub.status.busy": "2024-11-09T06:06:40.329848Z",
     "iopub.status.idle": "2024-11-09T06:06:42.194164Z",
     "shell.execute_reply": "2024-11-09T06:06:42.193482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_batch: (256, 2000, 4, 1)\n",
      "Y_batch: (256,)\n"
     ]
    }
   ],
   "source": [
    "for batch in train.take(1):  \n",
    "    X_batch, Y_batch = batch\n",
    "    print(\"X_batch:\", X_batch.shape) \n",
    "    print(\"Y_batch:\", Y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e790a58c-8f5a-4168-88d1-e2d644724c7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T06:06:42.196461Z",
     "iopub.status.busy": "2024-11-09T06:06:42.196200Z",
     "iopub.status.idle": "2024-11-09T06:06:42.737356Z",
     "shell.execute_reply": "2024-11-09T06:06:42.736677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mini-batches: 173\n"
     ]
    }
   ],
   "source": [
    "num_batches = 0\n",
    "for _ in train:\n",
    "    num_batches += 1\n",
    "print(\"Number of mini-batches:\", num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c286f8d2-f28e-4a42-8d2d-295f2843a2a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T06:06:42.739709Z",
     "iopub.status.busy": "2024-11-09T06:06:42.739454Z",
     "iopub.status.idle": "2024-11-09T06:06:43.850086Z",
     "shell.execute_reply": "2024-11-09T06:06:43.849544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kelvi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\kelvi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Input(shape=(2000, 4, 1), batch_size=BATCH_SIZE))\n",
    "model.add(layers.Conv2D(32, (2, 2), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "model.add(layers.Conv2D(64, (2, 2), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Conv2D(128, (2, 2), activation='relu', padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.GlobalMaxPooling2D())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "230b4fe9-7cda-49cc-8359-998d4390f1ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T06:06:43.868058Z",
     "iopub.status.busy": "2024-11-09T06:06:43.867650Z",
     "iopub.status.idle": "2024-11-09T06:06:43.876479Z",
     "shell.execute_reply": "2024-11-09T06:06:43.876062Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss=tfa.losses.SigmoidFocalCrossEntropy(),  # Focal Loss from TensorFlow Addons\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3878734-1cd4-42ae-9067-3de26629be78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T06:06:43.878620Z",
     "iopub.status.busy": "2024-11-09T06:06:43.878319Z",
     "iopub.status.idle": "2024-11-09T07:22:41.525425Z",
     "shell.execute_reply": "2024-11-09T07:22:41.524905Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train,                  \n",
    "    epochs=EPOCH_SIZE,        \n",
    "    validation_data=dev     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e2a772",
   "metadata": {},
   "source": [
    "CNN+XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39702728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Input(shape=(2000, 4, 1)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 1), padding='same'),  # attention: collapsing width!\n",
    "    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 1), padding='same'),\n",
    "    layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.GlobalMaxPooling2D(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.3)\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer=tfa.optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-5)\n",
    "                  , loss=tfa.losses.SigmoidFocalCrossEntropy(), metrics=['accuracy'])\n",
    "cnn_model.fit(X_train, Y_train, epochs=3, batch_size=64, validation_data=(X_dev, Y_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f982d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1381/1381 [==============================] - 178s 127ms/step\n",
      "346/346 [==============================] - 54s 155ms/step\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = models.Model(inputs=cnn_model.input, outputs=cnn_model.layers[-2].output)\n",
    "X_train_features = feature_extractor.predict(X_train)\n",
    "X_dev_features = feature_extractor.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edca2cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(max_iter=100)\n",
    "classifier.fit(X_train_features, Y_train)\n",
    "\n",
    "Y_pred = classifier.predict(X_dev_features)\n",
    "Y_pred_proba = classifier.predict_proba(X_dev_features)[:, 1] \n",
    "\n",
    "accuracy = accuracy_score(Y_dev, Y_pred)\n",
    "precision = precision_score(Y_dev, Y_pred)\n",
    "recall = recall_score(Y_dev, Y_pred)\n",
    "f1 = f1_score(Y_dev, Y_pred)\n",
    "auc = roc_auc_score(Y_dev, Y_pred_proba)\n",
    "\n",
    "print(\"\\nEvaluation Metrics (Logistic Regression on CNN Features):\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54e69ef",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89399c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_num_boost_round = 100\n",
    "dtrain = xgb.DMatrix(X_train_flat, label=Y_train)\n",
    "ddev = xgb.DMatrix(X_dev_flat, label=Y_dev)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'learning_rate': 0.05,\n",
    "    'scale_pos_weight': sum(Y_train == 0) / sum(Y_train == 1) * 2,\n",
    "    'eval_metric': 'aucpr'\n",
    "}\n",
    "\n",
    "# The scale pos weight is also set dynamicaly based on the dataset\n",
    "\n",
    "max_depth_range = [3, 5, 7, 9]\n",
    "min_child_weight_range = [1, 2, 3, 5]\n",
    "best_max_depth, best_min_child_weight = None, None\n",
    "best_cv_score = float('-inf')\n",
    "\n",
    "for max_depth in max_depth_range:\n",
    "    for min_child_weight in min_child_weight_range:\n",
    "        params['max_depth'] = max_depth\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "\n",
    "        cv_results = xgb.cv(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=initial_num_boost_round,\n",
    "            nfold=3,\n",
    "            metrics='aucpr',\n",
    "            early_stopping_rounds=10,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        mean_aucpr = cv_results['test-aucpr-mean'].max()\n",
    "        if mean_aucpr > best_cv_score:\n",
    "            best_cv_score = mean_aucpr\n",
    "            best_max_depth = max_depth\n",
    "            best_min_child_weight = min_child_weight\n",
    "\n",
    "params['max_depth'] = best_max_depth\n",
    "params['min_child_weight'] = best_min_child_weight\n",
    "\n",
    "gamma_range = [0, 0.1, 0.2, 0.3, 0.4]\n",
    "best_gamma = None\n",
    "best_cv_score = float('-inf')\n",
    "\n",
    "for gamma in gamma_range:\n",
    "    params['gamma'] = gamma\n",
    "\n",
    "    cv_results = xgb.cv(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=initial_num_boost_round,\n",
    "        nfold=3,\n",
    "        metrics='aucpr',\n",
    "        early_stopping_rounds=10,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    mean_aucpr = cv_results['test-aucpr-mean'].max()\n",
    "    if mean_aucpr > best_cv_score:\n",
    "        best_cv_score = mean_aucpr\n",
    "        best_gamma = gamma\n",
    "\n",
    "params['gamma'] = best_gamma\n",
    "\n",
    "subsample_range = [0.6, 0.8, 1.0]\n",
    "colsample_bytree_range = [0.6, 0.8, 1.0]\n",
    "best_subsample, best_colsample_bytree = None, None\n",
    "best_cv_score = float('-inf')\n",
    "\n",
    "for subsample in subsample_range:\n",
    "    for colsample_bytree in colsample_bytree_range:\n",
    "        params['subsample'] = subsample\n",
    "        params['colsample_bytree'] = colsample_bytree\n",
    "\n",
    "        cv_results = xgb.cv(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=initial_num_boost_round,\n",
    "            nfold=3,\n",
    "            metrics='aucpr',\n",
    "            early_stopping_rounds=10,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        mean_aucpr = cv_results['test-aucpr-mean'].max()\n",
    "        if mean_aucpr > best_cv_score:\n",
    "            best_cv_score = mean_aucpr\n",
    "            best_subsample = subsample\n",
    "            best_colsample_bytree = colsample_bytree\n",
    "\n",
    "params['subsample'] = best_subsample\n",
    "params['colsample_bytree'] = best_colsample_bytree\n",
    "\n",
    "reg_alpha_range = [0, 0.1, 0.5, 0.8, 1.0]\n",
    "reg_lambda_range = [0.5, 1.0, 1.5, 2.0]\n",
    "best_reg_alpha, best_reg_lambda = None, None\n",
    "best_cv_score = float('-inf')\n",
    "\n",
    "for reg_alpha in reg_alpha_range:\n",
    "    for reg_lambda in reg_lambda_range:\n",
    "        params['reg_alpha'] = reg_alpha\n",
    "        params['reg_lambda'] = reg_lambda\n",
    "\n",
    "        cv_results = xgb.cv(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=initial_num_boost_round,\n",
    "            nfold=3,\n",
    "            metrics='aucpr',\n",
    "            early_stopping_rounds=10,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        mean_aucpr = cv_results['test-aucpr-mean'].max()\n",
    "        if mean_aucpr > best_cv_score:\n",
    "            best_cv_score = mean_aucpr\n",
    "            best_reg_alpha = reg_alpha\n",
    "            best_reg_lambda = reg_lambda\n",
    "\n",
    "params['reg_alpha'] = best_reg_alpha\n",
    "params['reg_lambda'] = best_reg_lambda\n",
    "\n",
    "learning_rate_range = [0.001, 0.01, 0.1]\n",
    "num_boost_round_range = [50, 100, 250]\n",
    "best_learning_rate, best_num_boost_round = None, None\n",
    "best_cv_score = float('-inf')\n",
    "\n",
    "for learning_rate in learning_rate_range:\n",
    "    for num_boost_round in num_boost_round_range:\n",
    "        params['learning_rate'] = learning_rate\n",
    "\n",
    "        cv_results = xgb.cv(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=num_boost_round,\n",
    "            nfold=3,\n",
    "            metrics='aucpr',\n",
    "            early_stopping_rounds=10,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        mean_aucpr = cv_results['test-aucpr-mean'].max()\n",
    "        if mean_aucpr > best_cv_score:\n",
    "            best_cv_score = mean_aucpr\n",
    "            best_learning_rate = learning_rate\n",
    "            best_num_boost_round = num_boost_round\n",
    "\n",
    "params['learning_rate'] = best_learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 7,\n",
    "    'min_child_weight': 2,\n",
    "    'gamma': 0.16666666666666666,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'reg_alpha': 0.5,\n",
    "    'reg_lambda': 1.0,\n",
    "    'scale_pos_weight': sum(Y_train == 0) / sum(Y_train == 1)*2,  # This will still be computed based on your data\n",
    "    'eval_metric': 'aucpr'\n",
    "}\n",
    "\n",
    "# The hyperparameters tuning results are also related to the dataset and down sampling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd20aa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = [(ddev, 'eval')]\n",
    "best_xgb_model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=100,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=True\n",
    ")\n",
    "\n",
    "Y_pred_proba = best_xgb_model.predict(ddev)\n",
    "Y_pred = (Y_pred_proba > 0.6).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc84329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "Accuracy: 0.6786\n",
      "Precision: 0.6809\n",
      "Recall: 0.7273\n",
      "F1 Score: 0.7033\n",
      "AUC: 0.7955\n"
     ]
    }
   ],
   "source": [
    "# Calculate Evaluation Metrics\n",
    "accuracy = accuracy_score(Y_dev, Y_pred)\n",
    "precision = precision_score(Y_dev, Y_pred)\n",
    "recall = recall_score(Y_dev, Y_pred)\n",
    "f1 = f1_score(Y_dev, Y_pred)\n",
    "auc = roc_auc_score(Y_dev, Y_pred_proba)\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "\"\"\"\n",
    "Evaluation Metrics:\n",
    "Accuracy: 0.6786\n",
    "Precision: 0.6809\n",
    "Recall: 0.7273\n",
    "F1 Score: 0.7033\n",
    "AUC: 0.7955\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714cec03",
   "metadata": {},
   "source": [
    "GAN (also covered in other codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7719fe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(2000, 4, 1)))  # Input shape should match the real data shape\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(128, (2, 2), strides=(1, 1), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(64, (2, 2), strides=(1, 1), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(1, (2, 2), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(2000, 4, 1)))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (2, 2), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(128, (2, 2), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2)) \n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.GlobalMaxPooling2D())\n",
    "    model.add(layers.Dense(1))  # Outputting a single score (real or fake)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the models\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)  # Non-saturating loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a98176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_data, real_labels):\n",
    "    noise = tf.random.normal([BATCH_SIZE, 2000, 4, 1])\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_data = generator(noise, training=True)\n",
    "        generated_labels = tf.zeros([BATCH_SIZE], dtype=tf.int64)  # Labels for generated data\n",
    "        \n",
    "        real_output = discriminator(real_data, training=True)\n",
    "        fake_output = discriminator(generated_data, training=True)\n",
    "        \n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for data_batch, label_batch in dataset:\n",
    "            train_step(data_batch, label_batch)\n",
    "        print(f'Epoch {epoch + 1}/{epochs} completed')\n",
    "\n",
    "train(train_dataset, 100) # 100 epochs\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_data = generator(noise, training=False)\n",
    "\n",
    "print('Generated Data:', np.squeeze(generated_data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cbf332a-4d6c-4d6c-804e-3172e59291c0",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48089e68-9a88-4e7c-8e83-140b15b45d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9b540-bdd6-430a-9ebd-f38712a8c00d",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d76b66-b7fc-4ced-a6db-f25333a0f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../../result/one_hot_encoding/gene_id_label_ohe.pkl.gz', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bacd75c8-2aa1-43f4-bd7f-58743537c0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ensembl_gene_id</th>\n",
       "      <th>DE</th>\n",
       "      <th>upstream_region_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000000457</td>\n",
       "      <td>0</td>\n",
       "      <td>[[1, 0, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000000460</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000000938</td>\n",
       "      <td>0</td>\n",
       "      <td>[[1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSG00000000971</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSG00000001460</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0, 0, 0, 1], [0, 0, 0, 1], [1, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55221</th>\n",
       "      <td>ENSG00000284520</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0, 1, 0, 0], [0, 0, 0, 1], [0, 1, 0, 0], [1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55222</th>\n",
       "      <td>ENSG00000284544</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 0, 1], [1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55223</th>\n",
       "      <td>ENSG00000284554</td>\n",
       "      <td>0</td>\n",
       "      <td>[[1, 0, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55224</th>\n",
       "      <td>ENSG00000284568</td>\n",
       "      <td>0</td>\n",
       "      <td>[[1, 0, 0, 0], [0, 0, 0, 1], [1, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55225</th>\n",
       "      <td>ENSG00000284583</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 0, 1], [0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55223 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ensembl_gene_id  DE                            upstream_region_encoded\n",
       "0      ENSG00000000457   0  [[1, 0, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0,...\n",
       "1      ENSG00000000460   0  [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [0,...\n",
       "2      ENSG00000000938   0  [[1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [1,...\n",
       "3      ENSG00000000971   1  [[0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0,...\n",
       "4      ENSG00000001460   0  [[0, 0, 0, 1], [0, 0, 0, 1], [1, 0, 0, 0], [0,...\n",
       "...                ...  ..                                                ...\n",
       "55221  ENSG00000284520   0  [[0, 1, 0, 0], [0, 0, 0, 1], [0, 1, 0, 0], [1,...\n",
       "55222  ENSG00000284544   0  [[0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 0, 1], [1,...\n",
       "55223  ENSG00000284554   0  [[1, 0, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0,...\n",
       "55224  ENSG00000284568   0  [[1, 0, 0, 0], [0, 0, 0, 1], [1, 0, 0, 0], [0,...\n",
       "55225  ENSG00000284583   0  [[0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 0, 1], [0,...\n",
       "\n",
       "[55223 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e581a5-d658-4023-b2a6-95548e7273a6",
   "metadata": {},
   "source": [
    "## Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b887a4e-574b-4b17-80d9-b48bfb589211",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack(data['upstream_region_encoded'].values)\n",
    "Y = data['DE'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81d9b8bd-4c79-4f38-a209-9be2fb5b0d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55223, 2000, 4)\n",
      "(55223,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a39f9d-2702-4921-a7f1-76f8a5be205c",
   "metadata": {},
   "source": [
    "## Train-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c797524-27b8-47cc-a5e1-eed6196a5542",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, Y_train, Y_dev = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2e8f62-62fb-4d8a-a666-9dd27bacd390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44178, 2000, 4)\n",
      "(11045, 2000, 4)\n",
      "(44178,)\n",
      "(11045,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81208e8-f0c3-4785-a182-9c4d208ba524",
   "metadata": {},
   "source": [
    "## Prepare Training Data for Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19f2311d-b53b-439f-9c3f-5963452cee52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3342, 2000, 4)\n"
     ]
    }
   ],
   "source": [
    "# get only the positive train data for training generative model\n",
    "\n",
    "X_train_positive = X_train[Y_train==1]\n",
    "print(X_train_positive.shape)\n",
    "\n",
    "gen_train_data = torch.from_numpy(X_train_positive).float().permute(0,2,1)\n",
    "gen_train_dataset = torch.utils.data.TensorDataset(gen_train_data)\n",
    "gen_train_dataloader = torch.utils.data.DataLoader(gen_train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d2eb2d-f48f-4a5d-8253-4b0cd4097975",
   "metadata": {},
   "source": [
    "## Define Gen Model and Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0df27d8-086d-4160-b1e2-1ab670e35f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, t):\n",
    "        t = t.float().view(-1, 1)\n",
    "        return self.layers(t)\n",
    "\n",
    "class DNADiffusionModel(nn.Module):\n",
    "    def __init__(self, sequence_length=2000, channels=4, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.channels = channels\n",
    "        \n",
    "        self.time_embed = TimeEmbedding(hidden_dim)\n",
    "        \n",
    "        # 1D CNN layers\n",
    "        self.input_conv = nn.Conv1d(channels, hidden_dim, 3, padding=1)\n",
    "        \n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.Conv1d(hidden_dim, hidden_dim, 3, padding=1),\n",
    "                nn.Conv1d(hidden_dim, hidden_dim, 3, padding=1)\n",
    "            ]) for _ in range(4)\n",
    "        ])\n",
    "        \n",
    "        self.output_conv = nn.Conv1d(hidden_dim, channels, 3, padding=1)\n",
    "        self.norm_layers = nn.ModuleList([\n",
    "            nn.GroupNorm(8, hidden_dim) for _ in range(9)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # x shape: (B,C,L) C is 4 in our case\n",
    "        # t shape: (B,)\n",
    "        \n",
    "        # Time embedding\n",
    "        t_emb = self.time_embed(t)\n",
    "        t_emb = t_emb.unsqueeze(-1).expand(-1, -1, self.sequence_length)\n",
    "        \n",
    "        h = self.input_conv(x)\n",
    "        h = F.silu(self.norm_layers[0](h))\n",
    "        h = h + t_emb\n",
    "        \n",
    "        # Residual blocks\n",
    "        norm_idx = 1  # Start from index 1 as 0 was used for input conv\n",
    "        for i, (conv1, conv2) in enumerate(self.conv_blocks):\n",
    "            residual = h\n",
    "            h = F.silu(self.norm_layers[norm_idx](conv1(h)))\n",
    "            h = F.silu(self.norm_layers[norm_idx + 1](conv2(h)))\n",
    "            h = residual + h\n",
    "            h = h + t_emb\n",
    "            norm_idx += 2  # Increment by 2 since we used 2 norm layers\n",
    "        \n",
    "        out = self.output_conv(h)\n",
    "        return out\n",
    "\n",
    "class DiffusionTrainer:\n",
    "    def __init__(self, sequence_length=2000, channels=4, timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.channels = channels\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps).cuda()\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
    "        \n",
    "    def add_noise(self, x, t):\n",
    "        noise = torch.randn_like(x).cuda()\n",
    "        sqrt_alpha_cumprod = self.sqrt_alphas_cumprod[t].view(-1, 1, 1)\n",
    "        sqrt_one_minus_alpha_cumprod = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1)\n",
    "        \n",
    "        return sqrt_alpha_cumprod * x + sqrt_one_minus_alpha_cumprod * noise, noise\n",
    "    \n",
    "    def sample(self, model, batch_size, device='cuda'):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn(batch_size, self.channels, self.sequence_length).cuda()\n",
    "            \n",
    "            for t in range(self.timesteps - 1, -1, -1):\n",
    "                t_batch = torch.full((batch_size,), t, dtype=torch.long).cuda()\n",
    "                predicted_noise = model(x, t_batch)\n",
    "                \n",
    "                alpha = self.alphas[t]\n",
    "                alpha_cumprod = self.alphas_cumprod[t]\n",
    "                beta = self.betas[t]\n",
    "                \n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(x).cuda()\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x).cuda()\n",
    "                    \n",
    "                x = (1 / torch.sqrt(alpha)) * (\n",
    "                    x - ((1 - alpha) / torch.sqrt(1 - alpha_cumprod)) * predicted_noise\n",
    "                ) + torch.sqrt(beta) * noise\n",
    "\n",
    "                # # Following might be unnecessary\n",
    "                # # After every 100 step, convert to one-hot-like representation\n",
    "                # if t % 100 == 0:  \n",
    "                #     x = F.softmax(x / 0.1, dim=1)  # Parameter 0.1\n",
    "                    \n",
    "            x = F.one_hot(x.argmax(dim=1), num_classes=self.channels).permute(0, 2, 1).float()\n",
    "        return x\n",
    "\n",
    "def train_diffusion(model, trainer, train_loader, optimizer, num_epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            x = batch[0].cuda()\n",
    "            \n",
    "            t = torch.randint(0, trainer.timesteps, (x.shape[0],), dtype=torch.long).cuda()\n",
    "            \n",
    "            noisy_x, target_noise = trainer.add_noise(x, t)\n",
    "            \n",
    "            predicted_noise = model(noisy_x, t)\n",
    "            \n",
    "            loss = F.mse_loss(predicted_noise, target_noise)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd50a4ac-ba62-4e56-a02e-6ee537fe04b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 6569.5255\n",
      "Epoch 2, Average Loss: 13.3280\n",
      "Epoch 3, Average Loss: 12.2871\n",
      "Epoch 4, Average Loss: 11.3294\n",
      "Epoch 5, Average Loss: 10.7546\n",
      "Epoch 6, Average Loss: 10.0462\n",
      "Epoch 7, Average Loss: 9.5349\n",
      "Epoch 8, Average Loss: 8.9760\n",
      "Epoch 9, Average Loss: 8.3820\n",
      "Epoch 10, Average Loss: 7.5747\n",
      "Epoch 11, Average Loss: 7.0177\n",
      "Epoch 12, Average Loss: 6.5203\n",
      "Epoch 13, Average Loss: 6.0056\n",
      "Epoch 14, Average Loss: 5.5307\n",
      "Epoch 15, Average Loss: 4.8236\n",
      "Epoch 16, Average Loss: 4.6197\n",
      "Epoch 17, Average Loss: 4.0778\n",
      "Epoch 18, Average Loss: 3.6245\n",
      "Epoch 19, Average Loss: 2.9283\n",
      "Epoch 20, Average Loss: 2.5272\n",
      "Epoch 21, Average Loss: 2.1937\n",
      "Epoch 22, Average Loss: 1.9475\n",
      "Epoch 23, Average Loss: 1.7547\n",
      "Epoch 24, Average Loss: 1.5951\n",
      "Epoch 25, Average Loss: 1.4241\n",
      "Epoch 26, Average Loss: 1.2691\n",
      "Epoch 27, Average Loss: 1.1465\n",
      "Epoch 28, Average Loss: 1.0391\n",
      "Epoch 29, Average Loss: 0.9072\n",
      "Epoch 30, Average Loss: 0.7879\n",
      "Epoch 31, Average Loss: 0.6687\n",
      "Epoch 32, Average Loss: 0.6704\n",
      "Epoch 33, Average Loss: 0.7101\n",
      "Epoch 34, Average Loss: 0.5283\n",
      "Epoch 35, Average Loss: 0.5427\n",
      "Epoch 36, Average Loss: 1.8729\n",
      "Epoch 37, Average Loss: 0.4347\n",
      "Epoch 38, Average Loss: 0.3668\n",
      "Epoch 39, Average Loss: 0.3702\n",
      "Epoch 40, Average Loss: 4.9432\n",
      "Epoch 41, Average Loss: 0.5562\n",
      "Epoch 42, Average Loss: 0.2985\n",
      "Epoch 43, Average Loss: 0.2866\n",
      "Epoch 44, Average Loss: 0.2704\n",
      "Epoch 45, Average Loss: 0.2825\n",
      "Epoch 46, Average Loss: 0.3000\n",
      "Epoch 47, Average Loss: 0.2549\n",
      "Epoch 48, Average Loss: 0.2481\n",
      "Epoch 49, Average Loss: 17.0496\n",
      "Epoch 50, Average Loss: 0.9742\n",
      "Epoch 51, Average Loss: 0.2389\n",
      "Epoch 52, Average Loss: 0.2241\n",
      "Epoch 53, Average Loss: 0.2137\n",
      "Epoch 54, Average Loss: 0.2388\n",
      "Epoch 55, Average Loss: 0.2109\n",
      "Epoch 56, Average Loss: 0.2293\n",
      "Epoch 57, Average Loss: 4.0402\n",
      "Epoch 58, Average Loss: 9.4027\n",
      "Epoch 59, Average Loss: 3.2873\n",
      "Epoch 60, Average Loss: 0.2207\n",
      "Epoch 61, Average Loss: 0.1912\n",
      "Epoch 62, Average Loss: 0.1962\n",
      "Epoch 63, Average Loss: 0.1938\n",
      "Epoch 64, Average Loss: 0.1827\n",
      "Epoch 65, Average Loss: 0.1907\n",
      "Epoch 66, Average Loss: 0.2269\n",
      "Epoch 67, Average Loss: 0.3751\n",
      "Epoch 68, Average Loss: 2.4039\n",
      "Epoch 69, Average Loss: 0.4999\n",
      "Epoch 70, Average Loss: 1.1792\n",
      "Epoch 71, Average Loss: 1.8821\n",
      "Epoch 72, Average Loss: 8.8813\n",
      "Epoch 73, Average Loss: 0.2539\n",
      "Epoch 74, Average Loss: 1.3167\n",
      "Epoch 75, Average Loss: 1.0364\n",
      "Epoch 76, Average Loss: 0.7992\n",
      "Epoch 77, Average Loss: 4.9810\n",
      "Epoch 78, Average Loss: 3.1128\n",
      "Epoch 79, Average Loss: 0.1820\n",
      "Epoch 80, Average Loss: 0.1935\n",
      "Epoch 81, Average Loss: 0.2551\n",
      "Epoch 82, Average Loss: 0.5012\n",
      "Epoch 83, Average Loss: 7.2507\n",
      "Epoch 84, Average Loss: 4.6071\n",
      "Epoch 85, Average Loss: 13.0476\n",
      "Epoch 86, Average Loss: 0.1980\n",
      "Epoch 87, Average Loss: 0.1799\n",
      "Epoch 88, Average Loss: 0.1701\n",
      "Epoch 89, Average Loss: 0.1696\n",
      "Epoch 90, Average Loss: 0.1723\n",
      "Epoch 91, Average Loss: 0.1726\n",
      "Epoch 92, Average Loss: 0.1960\n",
      "Epoch 93, Average Loss: 4.3639\n",
      "Epoch 94, Average Loss: 0.1764\n",
      "Epoch 95, Average Loss: 0.3035\n",
      "Epoch 96, Average Loss: 0.4337\n",
      "Epoch 97, Average Loss: 1.0499\n",
      "Epoch 98, Average Loss: 9.3988\n",
      "Epoch 99, Average Loss: 1.2187\n",
      "Epoch 100, Average Loss: 0.7352\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DNADiffusionModel().to(device)\n",
    "trainer = DiffusionTrainer()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "train_diffusion(model, trainer, gen_train_dataloader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc0e63e0-20e7-47c2-a3a4-5ed6d8ca7b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 110/110 [2:12:46<00:00, 72.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate 500*110 = 55,000 fake positive sequences\n",
    "from tqdm import tqdm\n",
    "generated_data = []\n",
    "for _ in tqdm(range(110)):\n",
    "    with torch.no_grad():\n",
    "        new_sequences = trainer.sample(model, batch_size=500, device=device)\n",
    "        generated_data.append(new_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c344c91-1af7-434b-adfa-9224def5e42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([55000, 4, 2000])\n"
     ]
    }
   ],
   "source": [
    "generated_data = torch.stack(generated_data).view(-1,4,2000)\n",
    "print(generated_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f45fbca2-6cdb-49b9-97e5-3591bbc1247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('generated_X.npz',X_gen=generated_data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33bd165-a35b-45ad-a7c8-ee08679bd1de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
